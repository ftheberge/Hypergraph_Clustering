{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook - modularity functions with HNX2\n",
    "\n",
    "We compare functions in the modularity module of HNX with new proposed versions;\n",
    "\n",
    "This is to be pulled into an upcoming version of HNX, currently in: https://github.com/ftheberge/HyperNetX/tree/modularity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import partition_igraph\n",
    "import hypernetx as hnx\n",
    "import hypernetx.algorithms.hypergraph_modularity as hmod ## new as of version 1.2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from scipy.special import comb\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "hnx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datadir = \"../Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates to HNX2.0 modularity\n",
    "\n",
    "### Unchanged functions:\n",
    "\n",
    "- dict2part(D)\n",
    "- part2dict(A)\n",
    "- linear(d, c)\n",
    "- majority(d, c)\n",
    "- strict(d, c)\n",
    "- two_section(HG)\n",
    "- kumar(HG, delta=0.01)\n",
    "\n",
    "### No longer required\n",
    "\n",
    "- precompute_attributes(H)\n",
    "- _compute_partition_probas(HG, A)\n",
    "- _degree_tax(HG, Pr, wdc)\n",
    "- _edge_contribution(HG, A, wdc)\n",
    "- _delta_ec(HG, P, v, a, b, wdc)\n",
    "- _bin_ppmf(d, c, p)\n",
    "- _delta_dt(HG, P, v, a, b, wdc)\n",
    "\n",
    "### New version (temporarily renamed with prefix “new_”)\n",
    "\n",
    "- modularity(HG, A, wdc=linear)\n",
    "- last_step(HG, L, wdc=linear, delta=0.01)\n",
    "\n",
    "### New functions\n",
    "\n",
    "- _last_step_unweighted\n",
    "- _last_step_weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New or updated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New proposed implementation for modularity\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import binom \n",
    "\n",
    "def new_modularity(H, A, wdc=hmod.linear):\n",
    "\n",
    "    ## all same edge weights?\n",
    "    uniq = (len(Counter(H.edges.properties['weight']))==1)\n",
    "    \n",
    "    ## Edge Contribution\n",
    "    H_id = H.incidence_dict\n",
    "    d = hmod.part2dict(A)\n",
    "    L = [ [d[i] for i in H_id[x]] for x in H_id ]\n",
    "\n",
    "    ## all same weight\n",
    "    if uniq:\n",
    "        _ctr = Counter([ (Counter(l).most_common(1)[0][1],len(l)) for l in L])\n",
    "        EC = sum([wdc(k[1],k[0])*_ctr[k] for k in _ctr.keys() if k[0] > k[1]/2])\n",
    "    else:\n",
    "        _keys = [ (Counter(l).most_common(1)[0][1],len(l)) for l in L]\n",
    "        _vals = list(H.edge_props['weight']) ## Thanks Brenda!!\n",
    "        _df = pd.DataFrame(zip(_keys,_vals), columns=['key','val'])\n",
    "        _df = _df.groupby(by='key').sum()\n",
    "        EC = sum([ wdc(k[1],k[0])*v[0] for (k,v) in _df.iterrows() if k[0]>k[1]/2 ])\n",
    "        \n",
    "    ## Degree Tax\n",
    "    if uniq:        \n",
    "        VolA = [sum([H.degree(i) for i in k]) for k in A]\n",
    "        Ctr = Counter([H.size(i) for i in H.edges])\n",
    "\n",
    "    else:\n",
    "        ## this is the bottleneck\n",
    "        VolA = np.repeat(0,1+np.max(list(d.values())))\n",
    "        m = np.max([H.size(i) for i in H.edges])\n",
    "        Ctr = np.repeat(0,1+m)\n",
    "        S = 0\n",
    "        for e in H.edges:\n",
    "            w = H.edges[e].weight\n",
    "            Ctr[H.size(e)] += w  \n",
    "            S += w\n",
    "            for v in H.edges[e]:\n",
    "                VolA[d[v]] += w \n",
    "                \n",
    "    VolV = np.sum(VolA)\n",
    "    VolA = [x/VolV for x in VolA]\n",
    "    DT = 0\n",
    "    \n",
    "    if uniq:        \n",
    "        for d in Ctr.keys():\n",
    "            Cnt = Ctr[d]\n",
    "            for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                for Vol in VolA:\n",
    "                    DT += (Cnt*wdc(d,c)*binom.pmf(c,d,Vol))\n",
    "        return (EC-DT)/H.number_of_edges()\n",
    "    else:\n",
    "        for d in range(len(Ctr)):\n",
    "            Cnt = Ctr[d]\n",
    "            for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                for Vol in VolA:\n",
    "                    DT += (Cnt*wdc(d,c)*binom.pmf(c,d,Vol))\n",
    "        return (EC-DT)/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS ASSUMES UNWEIGHTED H \n",
    "def _last_step_unweighted(H, A, wdc, delta=0.01):\n",
    "\n",
    "    qH = new_modularity(H,A,wdc=wdc)\n",
    "    print('initial qH:',qH)\n",
    "\n",
    "    ## initialize\n",
    "    ctr_sizes = Counter([H.size(i) for i in H.edges])\n",
    "    VolA = [sum([H.degree(i) for i in k]) for k in A]\n",
    "    VolV = np.sum(VolA)\n",
    "    dct_A = hmod.part2dict(A)\n",
    "\n",
    "    while(True):\n",
    "        \n",
    "        n_moves = 0\n",
    "        for v in list(np.random.permutation(list(H.nodes))):\n",
    "\n",
    "            dct_A_v = dct_A[v]\n",
    "            H_id = [H.incidence_dict[x] for x in H.nodes[v].memberships]\n",
    "            L = [ [dct_A[i] for i in x] for x in H_id ]\n",
    "            deg_v = H.degree(v)\n",
    "\n",
    "            ## assume unweighted - EC portion before\n",
    "            _ctr = Counter([ (Counter(l).most_common(1)[0][1],len(l)) for l in L])\n",
    "            ec = sum([wdc(k[1],k[0])*_ctr[k] for k in _ctr.keys() if k[0] > k[1]/2])\n",
    "\n",
    "            ## DT portion before\n",
    "            dt = 0\n",
    "            for d in ctr_sizes.keys():\n",
    "                Cnt = ctr_sizes[d]\n",
    "                for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                    dt += (Cnt*wdc(d,c)*binom.pmf(c,d,VolA[dct_A_v]/VolV)) \n",
    "            \n",
    "            ## move it?\n",
    "            best = dct_A_v\n",
    "            best_del_q = 0\n",
    "            best_dt = 0\n",
    "            for m in set([i for x in L for i in x])-{dct_A_v}:\n",
    "                dct_A[v] = m\n",
    "                L = [ [dct_A[i] for i in x] for x in H_id ]\n",
    "                ## assume unweighted - EC\n",
    "                _ctr = Counter([ (Counter(l).most_common(1)[0][1],len(l)) for l in L])\n",
    "                ecp = sum([wdc(k[1],k[0])*_ctr[k] for k in _ctr.keys() if k[0] > k[1]/2])\n",
    "                ## DT\n",
    "                del_dt = -dt\n",
    "                for d in ctr_sizes.keys():\n",
    "                    Cnt = ctr_sizes[d]\n",
    "                    for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                        del_dt -= (Cnt*wdc(d,c)*binom.pmf(c,d,VolA[m]/VolV))\n",
    "                        del_dt += (Cnt*wdc(d,c)*binom.pmf(c,d,(VolA[m]+deg_v)/VolV))\n",
    "                        del_dt += (Cnt*wdc(d,c)*binom.pmf(c,d,(VolA[dct_A_v]-deg_v)/VolV)) \n",
    "                del_q = ecp-ec-del_dt\n",
    "                if del_q > best_del_q:\n",
    "                    best_del_q = del_q\n",
    "                    best = m\n",
    "                    best_dt = del_dt\n",
    "            if best_del_q > 0.1: ## this avoids some numerical precision issues\n",
    "                n_moves += 1\n",
    "                dct_A[v] = best\n",
    "                VolA[m] += deg_v\n",
    "                VolA[dct_A_v] -= deg_v\n",
    "                VolV = np.sum(VolA)\n",
    "            else:\n",
    "                dct_A[v] = dct_A_v\n",
    "        new_qH = new_modularity(H, hmod.dict2part(dct_A), wdc=wdc)    \n",
    "        print(n_moves,'moves, new qH:',new_qH)\n",
    "        if (new_qH-qH) < delta:\n",
    "            break\n",
    "        else:\n",
    "            qH = new_qH\n",
    "    return hmod.dict2part(dct_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS ASSUMES WEIGHTED H\n",
    "def _last_step_weighted(H, A, wdc, delta=0.01):\n",
    "\n",
    "    qH = new_modularity(H,A,wdc=wdc)\n",
    "    print('initial qH:',qH)\n",
    "    d = hmod.part2dict(A)\n",
    "\n",
    "    ## initialize\n",
    "    ## this is the bottleneck\n",
    "    VolA = np.repeat(0,1+np.max(list(d.values())))\n",
    "    m = np.max([H.size(i) for i in H.edges])\n",
    "    ctr_sizes = np.repeat(0,1+m)\n",
    "    S = 0\n",
    "    for e in H.edges:\n",
    "        w = H.edges[e].weight\n",
    "        ctr_sizes[H.size(e)] += w  \n",
    "        S += w\n",
    "        for v in H.edges[e]:\n",
    "            VolA[d[v]] += w \n",
    "    VolV = np.sum(VolA)\n",
    "    dct_A = hmod.part2dict(A)\n",
    "\n",
    "    ## loop\n",
    "    while(True):\n",
    "        n_moves = 0\n",
    "        for v in list(np.random.permutation(list(H.nodes))):\n",
    "\n",
    "            dct_A_v = dct_A[v]\n",
    "            H_id = [H.incidence_dict[x] for x in H.nodes[v].memberships]\n",
    "            L = [ [dct_A[i] for i in x] for x in H_id ]\n",
    "\n",
    "            ## ec portion before move\n",
    "            _keys = [ (Counter(l).most_common(1)[0][1],len(l)) for l in L]\n",
    "            _vals = [H.edge_props['weight'][x] for x in H.nodes[v].memberships]\n",
    "            _df = pd.DataFrame(zip(_keys,_vals), columns=['key','val'])\n",
    "            _df = _df.groupby(by='key').sum()\n",
    "            ec = sum([ wdc(k[1],k[0])*val[0] for (k,val) in _df.iterrows() if k[0]>k[1]/2 ])\n",
    "            str_v = np.sum(_vals) ## weighted degree\n",
    "\n",
    "            ## DT portion before move\n",
    "            dt = 0\n",
    "            for d in range(len(ctr_sizes)):\n",
    "                Cnt = ctr_sizes[d]\n",
    "                for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                    dt += (Cnt*wdc(d,c)*binom.pmf(c,d,VolA[dct_A_v]/VolV)) \n",
    "\n",
    "\n",
    "            ## move it?\n",
    "            best = dct_A_v\n",
    "            best_del_q = 0\n",
    "            best_dt = 0\n",
    "            for m in set([i for x in L for i in x])-{dct_A_v}:\n",
    "                dct_A[v] = m\n",
    "                L = [ [dct_A[i] for i in x] for x in H_id ]\n",
    "                ## EC\n",
    "                _keys = [ (Counter(l).most_common(1)[0][1],len(l)) for l in L]\n",
    "                _vals = [H.edge_props['weight'][x] for x in H.nodes[v].memberships]\n",
    "                _df = pd.DataFrame(zip(_keys,_vals), columns=['key','val'])\n",
    "                _df = _df.groupby(by='key').sum()\n",
    "                ecp = sum([ wdc(k[1],k[0])*val[0] for (k,val) in _df.iterrows() if k[0]>k[1]/2 ])    \n",
    "\n",
    "\n",
    "                ## DT\n",
    "                del_dt = -dt\n",
    "                for d in range(len(ctr_sizes)):\n",
    "                    Cnt = ctr_sizes[d]\n",
    "                    for c in np.arange(int(np.floor(d/2+1)),d+1):\n",
    "                        del_dt -= (Cnt*wdc(d,c)*binom.pmf(c,d,VolA[m]/VolV))\n",
    "                        del_dt += (Cnt*wdc(d,c)*binom.pmf(c,d,(VolA[m]+str_v)/VolV))\n",
    "                        del_dt += (Cnt*wdc(d,c)*binom.pmf(c,d,(VolA[dct_A_v]-str_v)/VolV))\n",
    "                del_q = ecp-ec-del_dt\n",
    "                if del_q > best_del_q:\n",
    "                    best_del_q = del_q\n",
    "                    best = m\n",
    "                    best_dt = del_dt\n",
    "\n",
    "            if best_del_q > 0.1: ## this avoids some precision issues\n",
    "                n_moves += 1\n",
    "                dct_A[v] = best\n",
    "                VolA[m] += str_v\n",
    "                VolA[dct_A_v] -= str_v\n",
    "                VolV = np.sum(VolA)\n",
    "            else:\n",
    "                dct_A[v] = dct_A_v\n",
    "\n",
    "        new_qH = new_modularity(H, hmod.dict2part(dct_A), wdc=wdc)\n",
    "        print(n_moves,'moves, new qH:',new_qH)\n",
    "        if (new_qH-qH) < delta:\n",
    "            break\n",
    "        else:\n",
    "            qH = new_qH\n",
    "    return hmod.dict2part(dct_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_last_step(H, A, wdc=hmod.linear, delta=0.01):\n",
    "    \n",
    "    ## all same edge weights?\n",
    "    uniq = (len(Counter(H.edges.properties['weight']))==1)\n",
    "\n",
    "    if uniq:\n",
    "        nls = _last_step_unweighted(H, A, wdc=wdc, delta=delta)\n",
    "    else:\n",
    "        nls = _last_step_weighted(H, A, wdc=wdc, delta=delta)\n",
    "    return nls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with h-ABCD hypergraphs\n",
    "\n",
    "We generated 4 h-ABCD hypergraphs with parameters:\n",
    "\n",
    "* -n 1000 -d 2.5,5,50 -c 1.5,50,200 -x 0.5 -q 0.0,0.4,0.3,0.2,0.1 -w :**linear** -s 1234 -o linear_1000\n",
    "* same as above with **strict**, **majority**\n",
    "* -n 1000 -d 2.5,5,50 -c 1.5,50,200 -x 0.5 -q 0.0,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1 -w :linear -s 1234 -o linear_large_edges_1000\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3385)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_vertex_labels = Datadir+'linear_1000_assign.txt'\n",
    "file_hyperedges = Datadir+'linear_1000_he.txt'\n",
    "with open(file_hyperedges, 'r') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    lines = file.readlines()\n",
    "hyperedges = [[y for y in x.replace('\\n','').split(',')] for x in lines]\n",
    "\n",
    "with open(file_vertex_labels, 'r') as file:\n",
    "    # Read all the lines of the file into a list\n",
    "    vertex_labels = np.array([int(y) for y in file.read().splitlines()])\n",
    "\n",
    "H = hnx.Hypergraph(hyperedges)\n",
    "H.shape    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## optional - add random edge weights \n",
    "for e in H.edges:\n",
    "    H.edges[e].weight = np.random.choice(10)+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 176 ms, sys: 226 µs, total: 176 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Cluster the 2-section graph (with Louvain)\n",
    "G = hmod.two_section(H)\n",
    "G.vs['louvain'] = G.community_multilevel(weights='weight').membership\n",
    "ML = hmod.dict2part({v['name']:v['louvain'] for v in G.vs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 73.9 ms, total: 13.6 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## pre-compute required quantities for modularity and clustering with current HNX functions\n",
    "H = hmod.precompute_attributes(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.37731947885116807\n",
      "q-majority: 0.38903123340397894\n",
      "q-strict: 0.347003775016151\n",
      "CPU times: user 4.36 s, sys: 24.6 ms, total: 4.38 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with current HNX function\n",
    "print('q-linear:',hmod.modularity(H, ML, wdc=hmod.linear))\n",
    "print('q-majority:',hmod.modularity(H, ML, wdc=hmod.majority))\n",
    "print('q-strict:',hmod.modularity(H, ML, wdc=hmod.strict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.37731947885116546\n",
      "q-majority: 0.3890312334039788\n",
      "q-strict: 0.34700377501615104\n",
      "CPU times: user 114 ms, sys: 0 ns, total: 114 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with new function\n",
    "print('q-linear:',new_modularity(H, ML, wdc=hmod.linear))\n",
    "print('q-majority:',new_modularity(H, ML, wdc=hmod.majority))\n",
    "print('q-strict:',new_modularity(H, ML, wdc=hmod.strict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.7 s, sys: 18.6 ms, total: 7.72 s\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "KU = hmod.kumar(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.36809928524991276\n",
      "q-majority: 0.3812803723167444\n",
      "q-strict: 0.33497415987329315\n",
      "CPU times: user 4.31 s, sys: 24.4 ms, total: 4.34 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with current HNX function\n",
    "print('q-linear:',hmod.modularity(H, KU, wdc=hmod.linear))\n",
    "print('q-majority:',hmod.modularity(H, KU, wdc=hmod.majority))\n",
    "print('q-strict:',hmod.modularity(H, KU, wdc=hmod.strict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.36809928524991065\n",
      "q-majority: 0.38128037231674433\n",
      "q-strict: 0.3349741598732931\n",
      "CPU times: user 46.4 ms, sys: 3.49 ms, total: 49.9 ms\n",
      "Wall time: 48.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with new function\n",
    "print('q-linear:',new_modularity(H, KU, wdc=hmod.linear))\n",
    "print('q-majority:',new_modularity(H, KU, wdc=hmod.majority))\n",
    "print('q-strict:',new_modularity(H, KU, wdc=hmod.strict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial qH: 0.36809928524991065\n",
      "100 moves, new qH: 0.38514228504186865\n",
      "38 moves, new qH: 0.38932093928072026\n",
      "CPU times: user 22.1 s, sys: 132 ms, total: 22.2 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "KU_nls = new_last_step(H, KU, wdc=hmod.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.389320939280723\n",
      "q-majority: 0.41813771866779315\n",
      "q-strict: 0.32122263643404636\n",
      "CPU times: user 4.37 s, sys: 11.9 ms, total: 4.38 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with current HNX function\n",
    "print('q-linear:',hmod.modularity(H, KU_nls, wdc=hmod.linear))\n",
    "print('q-majority:',hmod.modularity(H, KU_nls, wdc=hmod.majority))\n",
    "print('q-strict:',hmod.modularity(H, KU_nls, wdc=hmod.strict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-linear: 0.38932093928072026\n",
      "q-majority: 0.41813771866779315\n",
      "q-strict: 0.32122263643404636\n",
      "CPU times: user 48.3 ms, sys: 41 µs, total: 48.4 ms\n",
      "Wall time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compute qH with new function\n",
    "print('q-linear:',new_modularity(H, KU_nls, wdc=hmod.linear))\n",
    "print('q-majority:',new_modularity(H, KU_nls, wdc=hmod.majority))\n",
    "print('q-strict:',new_modularity(H, KU_nls, wdc=hmod.strict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-graphmining]",
   "language": "python",
   "name": "conda-env-.conda-graphmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
